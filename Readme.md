数据清洗过程
Step1 转换汉字编码格式
因为汉字有不同编码格式，不同格式混用会导致乱码，所以第一步，先找到源文件最适合的编码格式



Step2 从csv文件中提取新闻的国际特征


运行代码divide.py，在设置处调整德意组还是日韩组，代码将根据csv文件中对应国家出现的词频进行国家分组


Step3 提取新闻关键词
TF-IDF 与 TextRank 双算法融合 的机制
词性限制，只允许提取动词、名词、地名、人名、机构名
利用停用词表拦截无意义高频词汇，利用固定搭配防止专业术语被错误分解，反复修改并更新停用词表和固定搭配，直至关键词都变为有实际意义的词汇
动态 TopK，如果文章极短（小于50字），强制只提 3 - 5 个；否则提 10 个。
运行代码TOP-K.py,生成TOP-K keyword文件夹，其下是每个txt文本的关键词

Step4 提取国家关键词
Zipf 定律（词频定律）：
在一个语料库中，真正有代表性的核心词汇通常只占总词汇量的 10%-20%。
对于 100 篇新闻，去重后的有效关键词总量可能在 1000-2000 左右，其中前 15% (即 200-300) 通常覆盖了 80% 的核心语义。
DF阈值：
规则：一个词必须至少在 3 个或 5 个 不同的文件中出现过，才有资格代表这个“国家”。理由：如果“某某村”只在 1 篇文章里出现，它是这篇文章的关键词，但不是这个国家的关键词。
此处采用Zipf 定律对不同国家词云关键词数进行动态调整，基于DF阈值进行优先选取，若满足DF阈值的词数不够再根据频率选取
运行word cloud.py生成词云和国家关键词